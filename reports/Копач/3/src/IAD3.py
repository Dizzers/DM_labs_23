import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import warnings

warnings.filterwarnings('ignore')

# ==================== –ß–ê–°–¢–¨ 1: –û–°–ù–û–í–ù–û–ï –ó–ê–î–ê–ù–ò–ï ====================
print("=" * 70)
print("–õ–ê–ë–û–†–ê–¢–û–†–ù–ê–Ø –†–ê–ë–û–¢–ê ‚Ññ3: –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô –° –ü–†–ï–î–û–ë–£–ß–ï–ù–ò–ï–ú –ò –ë–ï–ó")
print("=" * 70)

# 1. –ó–ê–ì–†–£–ó–ö–ê –ò –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•
print("\n1. –ó–ê–ì–†–£–ó–ö–ê –ò –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•")


def load_cardiotocography_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∫–∞—Ä–¥–∏–æ—Ç–æ–∫–æ–≥—Ä–∞—Ñ–∏–∏"""
    try:
        df = pd.read_excel('CTG.xls', sheet_name='Data', header=1)
        print("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ CTG.xls")

        # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df = df.dropna(axis=1, how='all')

        # –í—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        feature_columns = ['LB', 'AC', 'FM', 'UC', 'DL', 'DS', 'DP',
                           'ASTV', 'MSTV', 'ALTV', 'MLTV',
                           'Width', 'Min', 'Max', 'Nmax', 'Nzeros',
                           'Mode', 'Mean', 'Median', 'Variance', 'Tendency']

        available_features = [col for col in feature_columns if col in df.columns]
        target_col = 'NSP' if 'NSP' in df.columns else 'CLASS'

        # –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        df_clean = df[available_features + [target_col]].dropna()

        X = df_clean[available_features]
        y = df_clean[target_col] - 1  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ 0-based

        print(f"üìä –î–∞–Ω–Ω—ã–µ: {X.shape[0]} samples, {X.shape[1]} features")
        print(f"üéØ –ö–ª–∞—Å—Å—ã: {np.unique(y)}")
        print(f"üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {np.bincount(y)}")

        return X, y, available_features

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        return None, None, None


# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
X, y, feature_names = load_cardiotocography_data()

if X is None:
    print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
    exit()

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä—ã PyTorch
X_train_tensor = torch.FloatTensor(X_train_scaled)
y_train_tensor = torch.LongTensor(y_train.values)
X_test_tensor = torch.FloatTensor(X_test_scaled)
y_test_tensor = torch.LongTensor(y_test.values)

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

print(f"üìä –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö: {X_train_scaled.shape}")
print(f"üéØ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {np.bincount(y_train)}")
print(f"üî¢ –ö–ª–∞—Å—Å—ã: {np.unique(y_train)}")

# 2. –ë–ê–ó–û–í–ê–Ø –ú–û–î–ï–õ–¨ –ë–ï–ó –ü–†–ï–î–û–ë–£–ß–ï–ù–ò–Ø
print("\n" + "=" * 50)
print("2. –ë–ê–ó–û–í–ê–Ø –ú–û–î–ï–õ–¨ –ë–ï–ó –ü–†–ï–î–û–ë–£–ß–ï–ù–ò–Ø")


class NeuralNetwork(nn.Module):
    def __init__(self, input_dim, num_classes):
        super(NeuralNetwork, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, num_classes)
        )

    def forward(self, x):
        return self.network(x)


def train_and_evaluate_model(model, train_loader, test_loader, epochs=100, model_name="–ú–æ–¥–µ–ª—å"):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    train_losses = []
    test_accuracies = []

    print(f"\nüéØ –û–±—É—á–µ–Ω–∏–µ {model_name}...")
    for epoch in range(epochs):
        # –û–±—É—á–µ–Ω–∏–µ
        model.train()
        total_loss = 0
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        model.eval()
        correct = 0
        total = 0
        all_preds = []
        all_labels = []

        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                outputs = model(batch_x)
                _, predicted = torch.max(outputs.data, 1)
                total += batch_y.size(0)
                correct += (predicted == batch_y).sum().item()
                all_preds.extend(predicted.numpy())
                all_labels.extend(batch_y.numpy())

        accuracy = correct / total
        train_losses.append(total_loss / len(train_loader))
        test_accuracies.append(accuracy)

        if (epoch + 1) % 20 == 0:
            print(f'üìà –≠–ø–æ—Ö–∞ [{epoch + 1}/{epochs}], –ü–æ—Ç–µ—Ä–∏: {total_loss / len(train_loader):.4f}, '
                  f'–¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.4f}')

    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    final_accuracy = accuracy_score(all_labels, all_preds)
    final_f1 = f1_score(all_labels, all_preds, average='weighted')
    cm = confusion_matrix(all_labels, all_preds)

    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã {model_name}:")
    print(f"   ‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å: {final_accuracy:.4f}")
    print(f"   ‚úÖ F1-score: {final_f1:.4f}")

    return final_accuracy, final_f1, cm, train_losses, test_accuracies


# –û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
input_dim = X_train_scaled.shape[1]
num_classes = len(np.unique(y_train))

base_model = NeuralNetwork(input_dim, num_classes)
base_accuracy, base_f1, cm_base, base_train_losses, base_test_accuracies = train_and_evaluate_model(
    base_model, train_loader, test_loader, epochs=100, model_name="–ë–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (–±–µ–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è)"
)

# 3. –ú–û–î–ï–õ–¨ –° –ü–†–ï–î–û–ë–£–ß–ï–ù–ò–ï–ú –ê–í–¢–û–≠–ù–ö–û–î–ï–†–û–ú
print("\n" + "=" * 50)
print("3. –ú–û–î–ï–õ–¨ –° –ü–†–ï–î–û–ë–£–ß–ï–ù–ò–ï–ú –ê–í–¢–û–≠–ù–ö–û–î–ï–†–û–ú")


class Autoencoder(nn.Module):
    def __init__(self, input_dim, encoding_dim):
        super(Autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, encoding_dim),
            nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(encoding_dim, input_dim)
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded


class ImprovedAutoencoderPretrainer:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤"""

    def __init__(self, layer_dims):
        self.layer_dims = layer_dims
        self.autoencoders = []

    def pretrain_layer(self, X, input_dim, encoding_dim, epochs=50):
        """–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ —Å–ª–æ—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–º"""
        print(f"üîß –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ —Å–ª–æ—è: {input_dim} ‚Üí {encoding_dim}")

        autoencoder = Autoencoder(input_dim, encoding_dim)
        criterion = nn.MSELoss()
        optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)

        X_tensor = torch.FloatTensor(X)

        for epoch in range(epochs):
            autoencoder.train()
            total_loss = 0
            num_batches = 0

            # –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            for batch_idx in range(0, len(X_tensor), 32):
                batch = X_tensor[batch_idx:batch_idx + 32]
                optimizer.zero_grad()
                reconstructed = autoencoder(batch)
                loss = criterion(reconstructed, batch)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
                num_batches += 1

            avg_loss = total_loss / num_batches if num_batches > 0 else 0

            if (epoch + 1) % 20 == 0:
                print(f'   üìâ –≠–ø–æ—Ö–∞ [{epoch + 1}/{epochs}], –ü–æ—Ç–µ—Ä–∏: {avg_loss:.4f}')

        return autoencoder.encoder[0].weight.data.clone(), autoencoder.encoder[0].bias.data.clone()

    def pretrain_stack(self, X, epochs_per_layer=50):
        """–ü–æ—Å–ª–æ–π–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤"""
        print("üéØ –ù–∞—á–∞–ª–æ –ø–æ—Å–ª–æ–π–Ω–æ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤...")
        current_data = X

        for i, encoding_dim in enumerate(self.layer_dims):
            input_dim = current_data.shape[1]
            print(f"üìö –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ —Å–ª–æ—è {i + 1}: {input_dim} ‚Üí {encoding_dim}")

            weights, biases = self.pretrain_layer(current_data, input_dim, encoding_dim, epochs_per_layer)
            self.autoencoders.append((weights, biases))

            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ—è
            with torch.no_grad():
                linear_layer = nn.Linear(input_dim, encoding_dim)
                linear_layer.weight.data = weights
                linear_layer.bias.data = biases
                current_data = torch.relu(linear_layer(torch.FloatTensor(current_data))).numpy()

        print("‚úÖ –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        return self.autoencoders


class PretrainedNeuralNetwork(nn.Module):
    def __init__(self, input_dim, num_classes, autoencoders):
        super(PretrainedNeuralNetwork, self).__init__()

        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–∏ —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
        self.layer1 = nn.Linear(input_dim, 256)
        self.layer2 = nn.Linear(256, 128)
        self.layer3 = nn.Linear(128, 64)
        self.output_layer = nn.Linear(64, num_classes)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –∏–∑ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤
        if len(autoencoders) >= 3:
            self.layer1.weight.data = autoencoders[0][0].clone()
            self.layer1.bias.data = autoencoders[0][1].clone()

            self.layer2.weight.data = autoencoders[1][0].clone()
            self.layer2.bias.data = autoencoders[1][1].clone()

            self.layer3.weight.data = autoencoders[2][0].clone()
            self.layer3.bias.data = autoencoders[2][1].clone()

        self.relu = nn.ReLU()
        self.dropout1 = nn.Dropout(0.4)
        self.dropout2 = nn.Dropout(0.3)
        self.dropout3 = nn.Dropout(0.2)

    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.dropout1(x)
        x = self.relu(self.layer2(x))
        x = self.dropout2(x)
        x = self.relu(self.layer3(x))
        x = self.dropout3(x)
        x = self.output_layer(x)
        return x


# –ü–æ—Å–ª–æ–π–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ
layer_dims = [256, 128, 64]  # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ç–∞–∫–∞—è –∂–µ –∫–∞–∫ —É –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏
pretrainer = ImprovedAutoencoderPretrainer(layer_dims)
autoencoders = pretrainer.pretrain_stack(X_train_scaled, epochs_per_layer=50)

# –°–æ–∑–¥–∞–Ω–∏–µ –∏ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
pretrained_model = PretrainedNeuralNetwork(input_dim, num_classes, autoencoders)
pretrained_accuracy, pretrained_f1, cm_pretrained, pretrained_train_losses, pretrained_test_accuracies = train_and_evaluate_model(
    pretrained_model, train_loader, test_loader, epochs=100, model_name="–ú–æ–¥–µ–ª–∏ —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ–º (Autoencoder)"
)

# 4. –°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
print("\n" + "=" * 70)
print("4. –°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
print("=" * 70)

print(f"\nüìä –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê:")
print(f"{'–ú–µ—Ç—Ä–∏–∫–∞':<20} {'–ë–µ–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è':<18} {'–° –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ–º':<18} {'–†–∞–∑–Ω–∏—Ü–∞':<12}")
print(f"{'-' * 70}")
print(
    f"{'–¢–æ—á–Ω–æ—Å—Ç—å':<20} {base_accuracy:.4f}           {pretrained_accuracy:.4f}             {pretrained_accuracy - base_accuracy:+.4f}")
print(f"{'F1-score':<20} {base_f1:.4f}           {pretrained_f1:.4f}             {pretrained_f1 - base_f1:+.4f}")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# 1. –ú–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
axes[0, 0].set_title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫\n–ë–µ–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è', fontweight='bold')
sns.heatmap(cm_base, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])
axes[0, 0].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
axes[0, 0].set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')

axes[0, 1].set_title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫\n–° –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ–º', fontweight='bold')
sns.heatmap(cm_pretrained, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1])
axes[0, 1].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
axes[0, 1].set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')

# 2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏
axes[0, 2].set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏', fontweight='bold')
models = ['–ë–µ–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è', '–° –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ–º']
accuracies = [base_accuracy, pretrained_accuracy]
colors = ['lightcoral', 'lightgreen']
bars = axes[0, 2].bar(models, accuracies, color=colors, alpha=0.7)
axes[0, 2].set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
axes[0, 2].set_ylim(0, 1)
for bar, accuracy in zip(bars, accuracies):
    axes[0, 2].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
                    f'{accuracy:.3f}', ha='center', fontweight='bold')

# 3. –ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è - —Ç–æ—á–Ω–æ—Å—Ç—å
axes[1, 0].set_title('–¢–æ—á–Ω–æ—Å—Ç—å –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è', fontweight='bold')
axes[1, 0].plot(base_test_accuracies, label='–ë–µ–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è', linewidth=2)
axes[1, 0].plot(pretrained_test_accuracies, label='–° –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ–º', linewidth=2)
axes[1, 0].set_xlabel('–≠–ø–æ—Ö–∞')
axes[1, 0].set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)

# 4. –ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è - –ø–æ—Ç–µ—Ä–∏
axes[1, 1].set_title('–ü–æ—Ç–µ—Ä–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è', fontweight='bold')
axes[1, 1].plot(base_train_losses, label='–ë–µ–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è', linewidth=2)
axes[1, 1].plot(pretrained_train_losses, label='–° –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ–º', linewidth=2)
axes[1, 1].set_xlabel('–≠–ø–æ—Ö–∞')
axes[1, 1].set_ylabel('–ü–æ—Ç–µ—Ä–∏')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

# 5. –í—ã–≤–æ–¥—ã
improvement_acc = pretrained_accuracy - base_accuracy
improvement_f1 = pretrained_f1 - base_f1

conclusion_text = f"–í–´–í–û–î–´:\n\n"
conclusion_text += f"‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å —É–ª—É—á—à–∏–ª–∞—Å—å –Ω–∞: {improvement_acc:+.4f}\n"
conclusion_text += f"‚Ä¢ F1-score —É–ª—É—á—à–∏–ª—Å—è –Ω–∞: {improvement_f1:+.4f}\n\n"

if improvement_acc > 0:
    conclusion_text += "‚úÖ –ü–†–ï–î–û–ë–£–ß–ï–ù–ò–ï –≠–§–§–ï–ö–¢–ò–í–ù–û!\n"
    conclusion_text += "–ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —É–ª—É—á—à–∏–ª\n–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏."
else:
    conclusion_text += "‚ùå –ü–†–ï–î–û–ë–£–ß–ï–ù–ò–ï –ù–ï –î–ê–õ–û –£–õ–£–ß–®–ï–ù–ò–Ø\n"
    conclusion_text += "–í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ –±–∞–∑–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥\n–æ–∫–∞–∑–∞–ª—Å—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º."

axes[1, 2].text(0.1, 0.5, conclusion_text, fontsize=12, fontweight='bold',
                verticalalignment='center', transform=axes[1, 2].transAxes)
axes[1, 2].set_title('–ó–∞–∫–ª—é—á–µ–Ω–∏–µ', fontweight='bold')
axes[1, 2].axis('off')

plt.tight_layout()
plt.show()

# ==================== –ß–ê–°–¢–¨ 2: –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –î–ê–ù–ù–´–• ====================
print("\n" + "=" * 70)
print("–ß–ê–°–¢–¨ 2: –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –î–ê–ù–ù–´–• –° –ü–û–ú–û–©–¨–Æ –ê–í–¢–û–≠–ù–ö–û–î–ï–†–ê")
print("=" * 70)

# 1. –ê–í–¢–û–≠–ù–ö–û–î–ï–† –î–õ–Ø –ì–õ–ê–í–ù–´–• –ö–û–ú–ü–û–ù–ï–ù–¢
print("\n1. –ü–†–û–ï–¶–ò–†–û–í–ê–ù–ò–ï –î–ê–ù–ù–´–• –° –ü–û–ú–û–©–¨–Æ –ê–í–¢–û–≠–ù–ö–û–î–ï–†–ê")


class PCA_Autoencoder(nn.Module):
    """–ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç"""

    def __init__(self, input_dim, n_components):
        super(PCA_Autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, n_components)  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
        )
        self.decoder = nn.Sequential(
            nn.Linear(n_components, 32),
            nn.ReLU(),
            nn.Linear(32, 64),
            nn.ReLU(),
            nn.Linear(64, input_dim)
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded


def train_autoencoder_for_pca(X_train, n_components, epochs=100):
    """–û–±—É—á–µ–Ω–∏–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç"""
    autoencoder = PCA_Autoencoder(X_train.shape[1], n_components)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)

    print(f"üéØ –û–±—É—á–µ–Ω–∏–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ —Å {n_components} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏...")
    X_tensor = torch.FloatTensor(X_train)

    for epoch in range(epochs):
        autoencoder.train()
        total_loss = 0
        num_batches = 0

        for batch_idx in range(0, len(X_tensor), 32):
            batch = X_tensor[batch_idx:batch_idx + 32]
            optimizer.zero_grad()
            reconstructed = autoencoder(batch)
            loss = criterion(reconstructed, batch)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            num_batches += 1

        avg_loss = total_loss / num_batches if num_batches > 0 else 0

        if (epoch + 1) % 50 == 0:
            print(f'   üìâ –≠–ø–æ—Ö–∞ [{epoch + 1}/{epochs}], –ü–æ—Ç–µ—Ä–∏: {avg_loss:.4f}')

    return autoencoder


# –û–±—É—á–µ–Ω–∏–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –¥–ª—è 2D –∏ 3D –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
autoencoder_2d = train_autoencoder_for_pca(X_train_scaled, 2, epochs=100)
autoencoder_3d = train_autoencoder_for_pca(X_train_scaled, 3, epochs=100)

# –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ü–∏–π
with torch.no_grad():
    X_pca_2d = autoencoder_2d.encoder(torch.FloatTensor(X_test_scaled)).numpy()
    X_pca_3d = autoencoder_3d.encoder(torch.FloatTensor(X_test_scaled)).numpy()

# 2. t-SNE –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø
print("\n2. t-SNE –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø")

# t-SNE —Å 2 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏
tsne_2d = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)
X_tsne_2d = tsne_2d.fit_transform(X_test_scaled)

# t-SNE —Å 3 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏
tsne_3d = TSNE(n_components=3, random_state=42, perplexity=30, n_iter=1000)
X_tsne_3d = tsne_3d.fit_transform(X_test_scaled)

# 3. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
print("\n3. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")

# –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
fig = plt.figure(figsize=(20, 15))

# –¶–≤–µ—Ç–æ–≤–∞—è —Å—Ö–µ–º–∞ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤
class_names = ['–ù–æ—Ä–º–∞–ª—å–Ω—ã–π', '–ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–π', '–ü–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π']
colors = ['green', 'orange', 'red']

# 1. –ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä - 2D
ax1 = fig.add_subplot(2, 3, 1)
for i, color in enumerate(colors):
    mask = (y_test == i)
    ax1.scatter(X_pca_2d[mask, 0], X_pca_2d[mask, 1],
                c=color, label=class_names[i], alpha=0.7, s=50)
ax1.set_title('–ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä - 2 –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã\n(–ê–Ω–∞–ª–æ–≥ PCA)', fontweight='bold', fontsize=12)
ax1.set_xlabel('–ì–ª–∞–≤–Ω–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ 1')
ax1.set_ylabel('–ì–ª–∞–≤–Ω–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ 2')
ax1.legend()
ax1.grid(True, alpha=0.3)

# 2. –ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä - 3D
ax2 = fig.add_subplot(2, 3, 2, projection='3d')
for i, color in enumerate(colors):
    mask = (y_test == i)
    ax2.scatter(X_pca_3d[mask, 0], X_pca_3d[mask, 1], X_pca_3d[mask, 2],
                c=color, label=class_names[i], alpha=0.7, s=50)
ax2.set_title('–ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä - 3 –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã\n(–ê–Ω–∞–ª–æ–≥ PCA)', fontweight='bold', fontsize=12)
ax2.set_xlabel('–ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ 1')
ax2.set_ylabel('–ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ 2')
ax2.set_zlabel('–ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ 3')
ax2.legend()

# 3. t-SNE - 2D
ax3 = fig.add_subplot(2, 3, 3)
for i, color in enumerate(colors):
    mask = (y_test == i)
    ax3.scatter(X_tsne_2d[mask, 0], X_tsne_2d[mask, 1],
                c=color, label=class_names[i], alpha=0.7, s=50)
ax3.set_title('t-SNE - 2 –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã\n(–ù–µ–ª–∏–Ω–µ–π–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è)', fontweight='bold', fontsize=12)
ax3.set_xlabel('t-SNE 1')
ax3.set_ylabel('t-SNE 2')
ax3.legend()
ax3.grid(True, alpha=0.3)

# 4. t-SNE - 3D
ax4 = fig.add_subplot(2, 3, 4, projection='3d')
for i, color in enumerate(colors):
    mask = (y_test == i)
    ax4.scatter(X_tsne_3d[mask, 0], X_tsne_3d[mask, 1], X_tsne_3d[mask, 2],
                c=color, label=class_names[i], alpha=0.7, s=50)
ax4.set_title('t-SNE - 3 –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã\n(–ù–µ–ª–∏–Ω–µ–π–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è)', fontweight='bold', fontsize=12)
ax4.set_xlabel('t-SNE 1')
ax4.set_ylabel('t-SNE 2')
ax4.set_zlabel('t-SNE 3')
ax4.legend()

# 5. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ - 2D
ax5 = fig.add_subplot(2, 3, 5)
# –í—ã—á–∏—Å–ª—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –º–µ—Ç–æ–¥–∞
from sklearn.metrics import silhouette_score

silhouette_ae = silhouette_score(X_pca_2d, y_test) if len(np.unique(y_test)) > 1 else 0
silhouette_tsne = silhouette_score(X_tsne_2d, y_test) if len(np.unique(y_test)) > 1 else 0

methods = ['–ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä', 't-SNE']
scores = [silhouette_ae, silhouette_tsne]
colors_methods = ['blue', 'purple']

bars = ax5.bar(methods, scores, color=colors_methods, alpha=0.7)
ax5.set_title('–ö–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏\n(Silhouette Score)', fontweight='bold', fontsize=12)
ax5.set_ylabel('Silhouette Score')
for bar, score in zip(bars, scores):
    ax5.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
             f'{score:.3f}', ha='center', fontweight='bold')

# 6. –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤
ax6 = fig.add_subplot(2, 3, 6)


# –í—ã—á–∏—Å–ª—è–µ–º –º–µ–∂–∫–ª–∞—Å—Å–æ–≤—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
def calculate_class_separation(projection, labels):
    unique_labels = np.unique(labels)
    separations = []
    for i in range(len(unique_labels)):
        for j in range(i + 1, len(unique_labels)):
            class_i = projection[labels == unique_labels[i]]
            class_j = projection[labels == unique_labels[j]]
            # –°—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Ü–µ–Ω—Ç—Ä–∞–º–∏ –∫–ª–∞—Å—Å–æ–≤
            dist = np.linalg.norm(class_i.mean(axis=0) - class_j.mean(axis=0))
            separations.append(dist)
    return np.mean(separations) if separations else 0


sep_ae = calculate_class_separation(X_pca_2d, y_test)
sep_tsne = calculate_class_separation(X_tsne_2d, y_test)

methods_sep = ['–ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä', 't-SNE']
separations = [sep_ae, sep_tsne]

bars_sep = ax6.bar(methods_sep, separations, color=['lightblue', 'lightcoral'], alpha=0.7)
ax6.set_title('–°—Ä–µ–¥–Ω–µ–µ –º–µ–∂–∫–ª–∞—Å—Å–æ–≤–æ–µ\n—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ', fontweight='bold', fontsize=12)
ax6.set_ylabel('–†–∞—Å—Å—Ç–æ—è–Ω–∏–µ')
for bar, sep in zip(bars_sep, separations):
    ax6.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
             f'{sep:.2f}', ha='center', fontweight='bold')

plt.tight_layout()
plt.show()

# –§–ò–ù–ê–õ–¨–ù–´–ï –í–´–í–û–î–´
print("\n" + "=" * 70)
print("–§–ò–ù–ê–õ–¨–ù–´–ï –í–´–í–û–î–´ –õ–ê–ë–û–†–ê–¢–û–†–ù–û–ô –†–ê–ë–û–¢–´ ‚Ññ3")
print("=" * 70)

print(f"\nüìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–°–ù–û–í–ù–û–ì–û –ó–ê–î–ê–ù–ò–Ø:")
print(f"   ‚Ä¢ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (–±–µ–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è):")
print(f"     - –¢–æ—á–Ω–æ—Å—Ç—å: {base_accuracy:.4f}")
print(f"     - F1-score: {base_f1:.4f}")
print(f"   ‚Ä¢ –ú–æ–¥–µ–ª—å —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ–º (Autoencoder):")
print(f"     - –¢–æ—á–Ω–æ—Å—Ç—å: {pretrained_accuracy:.4f}")
print(f"     - F1-score: {pretrained_f1:.4f}")
print(f"   ‚Ä¢ –£–ª—É—á—à–µ–Ω–∏–µ: {improvement_acc:+.4f}")

print(f"\nüé® –†–ï–ó–£–õ–¨–¢–ê–¢–´ –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò:")
print(f"   ‚Ä¢ –ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ–∫ –≥–ª–∞–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã")
print(f"   ‚Ä¢ t-SNE –ø–æ–∫–∞–∑–∞–ª –Ω–µ–ª–∏–Ω–µ–π–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö")
print(f"   ‚Ä¢ –û–±–∞ –º–µ—Ç–æ–¥–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Ö–æ—Ä–æ—à—É—é —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç—å –∫–ª–∞—Å—Å–æ–≤")

print(f"\n‚úÖ –í–´–ü–û–õ–ù–ï–ù–ù–´–ï –ó–ê–î–ê–ß–ò:")
print(f"   1. ‚úÖ –û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (4+ —Å–ª–æ—è)")
print(f"   2. ‚úÖ –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º")
print(f"   3. ‚úÖ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å/–±–µ–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è")
print(f"   4. ‚úÖ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–º (2D –∏ 3D)")
print(f"   5. ‚úÖ t-SNE –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è (2D –∏ 3D)")
print(f"   6. ‚úÖ –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")

print(f"\nüéØ –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï:")
if improvement_acc > 0:
    print("   –ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª —Å–≤–æ—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å")
    print("   –≤ —É–ª—É—á—à–µ–Ω–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏.")
else:
    print("   –í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –Ω–µ –¥–∞–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è,")
    print("   —á—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–≤—è–∑–∞–Ω–æ —Å –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—è–º–∏ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã.")

print("\n" + "=" * 70)
print("–õ–ê–ë–û–†–ê–¢–û–†–ù–ê–Ø –†–ê–ë–û–¢–ê ‚Ññ3 –í–´–ü–û–õ–ù–ï–ù–ê –£–°–ü–ï–®–ù–û! üéâ")
print("=" * 70)